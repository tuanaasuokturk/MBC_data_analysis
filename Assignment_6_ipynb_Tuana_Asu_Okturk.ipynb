{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuanaasuokturk/MBC_data_analysis/blob/main/Assignment_6_ipynb_Tuana_Asu_Okturk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxFPNerzM0Bh"
      },
      "source": [
        "<h1 style=\"font-size: 2.5em; font-weight: bold; margin-top: 0.5em; margin-bottom: 0.5em;\">\n",
        " Assignment 6: Advanced topics in regression</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSIDupmNIUMz"
      },
      "source": [
        "**In this assignment, we will:**\n",
        "- study how the estimated weights of a regression model change when introducing a correlated regressor, and understand what that means for **introducing nuisance regressors**\n",
        "- study why a **regression model cannot include colinear regressors**\n",
        "- see how we **detect colinear or strongly correlated regressors** that could affect estimation of weights, using the **Variance Inflation Factor**\n",
        "- study how we can determine in advance if our sample size is adequate to find a statistical significant effect, using **simulation-based power analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8OvYDQWIUM1"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>Preparing for this assignment:</b>\n",
        "You can prepare for the assignment by watching the following videos:\n",
        "    \n",
        "<li> <a href=\"https://www.youtube.com/watch?v=TA5UJYRyju0\">this video</a> on multicollinearity and the Variance Inflation Factor. You can also have a look at <a href=\"https://www.youtube.com/watch?v=Cba9LJ9lS8s\">this other video</a> which presents the same concept at a slower pace.</li>\n",
        "<li> these two videos from StatQuest on power analysis: <a href=\"https://www.youtube.com/watch?v=UFhJefdVCjE\">first</a> about p-hacking and statistical power, <a href=\"https://www.youtube.com/watch?v=VX_M3tIyiYk\">second</a> about power analysis</li>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l0pKMrXIUM1"
      },
      "source": [
        "# Getting prepared: Load the numerical judgment dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfISE2B_IUM4"
      },
      "source": [
        "We will first use the **same dataset as in Assignment 1** (Simple statistics): the orientation judgment dataset of (Talluri et al.,*Current Biology*). Reminder: the paradigm is presented in Figure 1 of the [article](https://www.cell.com/action/showPdf?pii=S0960-9822%2818%2930982-5). The structure of each trial is the following:\n",
        "- first, a stimulus (here a sequence of oriented gratings)\n",
        "- a discrimination task on this first stimulus (is the sequence overall tilted more clockwise or counter-clockwise?)\n",
        "- then a second stimulus (another sequence of oriented gratings)\n",
        "- finally, a numerical judgment task (report the mean over the *two* sequences).\n",
        "\n",
        "Load the data as a Pandas dataframe (as in Assignment 1). `x1` now represents the average orientation in the first sequence, and correspondingly for `x2`. In this assignment we will ignore the impact of the discrimination task on the final judgment (which was the main focus of interest of the original study)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": true,
        "id": "Hf4urAd2IUM5",
        "outputId": "fbfadd71-9f9f-4786-ac17-6b61e1242bac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   x1  x2  xavg     estim  subj\n",
              "0 -10   0    -5  -5.21220     1\n",
              "1 -10 -10   -10  -8.21768     1\n",
              "2  20  10    15 -17.93416     1\n",
              "3 -20  10    -5 -13.42634     1\n",
              "4  20 -10     5   9.88556     1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22163ed5-7e79-43bd-8b8f-38604dc253b5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>xavg</th>\n",
              "      <th>estim</th>\n",
              "      <th>subj</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-10</td>\n",
              "      <td>0</td>\n",
              "      <td>-5</td>\n",
              "      <td>-5.21220</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-10</td>\n",
              "      <td>-10</td>\n",
              "      <td>-10</td>\n",
              "      <td>-8.21768</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>15</td>\n",
              "      <td>-17.93416</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-20</td>\n",
              "      <td>10</td>\n",
              "      <td>-5</td>\n",
              "      <td>-13.42634</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20</td>\n",
              "      <td>-10</td>\n",
              "      <td>5</td>\n",
              "      <td>9.88556</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22163ed5-7e79-43bd-8b8f-38604dc253b5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-22163ed5-7e79-43bd-8b8f-38604dc253b5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-22163ed5-7e79-43bd-8b8f-38604dc253b5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# import typical packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# load the data from github\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/wimmerlab/MBC_data_analysis/main/A1_Statistics/Rawdata_Perceptual_simple.csv\",sep=',')\n",
        "\n",
        "# uncomment if you prefer to load the data locally\n",
        "#df = pd.read_csv(\"../A1_Statistics\\Rawdata_Perceptual_simple.csv\",sep=',')\n",
        "\n",
        "# list of subjects\n",
        "subjects = np.unique(df.subj)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB4Fv9w9IUM-"
      },
      "source": [
        "# The impact of introducing correlated regressors\n",
        "We want to understand in general how regression weights for a certain regressor $x1$ change when we include into the model another regressor $x2$ that correlates with $x1$.\n",
        "We will illustrate this where $x1$ and $x2$ correspond to the two stimulus sequences in each trial (and the dependent variable is the orientation estimated by the subject). We will compare the model with the second stimulus as unique regressor with the model where both stimuli are included.\n",
        "But first, **let's check that these two stimuli are indeed correlated, using Pearson's correlation.** We will use `pearsonr` from the `scipy` package (note: `numpy` has its own function for computing the correlation - `corrcoef` - but it does not provide p-values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QJ-2WZrgIUM_",
        "outputId": "90c6d3d6-fb15-4e93-f919-a1035799c0d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson's correlation coefficient 0.19056305211467914\n",
            "Pearson's correlation p-value 5.6004045695098874e-118\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# compute Pearson's correlation coeff and corresponding p-values\n",
        "rho, corr_p = pearsonr(df['x1'], df['x2'])\n",
        "\n",
        "print(\"Pearson's correlation coefficient\", rho)\n",
        "print(\"Pearson's correlation p-value\", corr_p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8nzn2ATIUNA"
      },
      "source": [
        "**Are the first and second stimuli correlated? Why do you think that stimuli were generated that way?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi63nNgBIUNB"
      },
      "source": [
        "es—x1 and x2 are (weakly) positively correlated. Using Pearson’s r on the provided dataframe gives r ≈ 0.19 with p ≈ 5.6×10⁻¹¹⁸, so the association is small in size but statistically very reliable. With positive covariance, the variance of xavg is larger, giving better coverage of the mean-orientation axis and more power to relate the subject’s estimate (estim) to the stimuli while keeping single-sequence values in a realistic range. The correlation could also arise from a pairing rule (e.g., drawing both sequences from the same discrete set or same-sign bins) rather than independent sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYQRIfmKIUNC"
      },
      "source": [
        "**Now let's fit the linear regression model with both stimuli as regressors.** We will make the comparison with the data of a single subject, subject 14."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-04T22:46:53.929474Z",
          "start_time": "2022-02-04T22:46:53.866909Z"
        },
        "id": "dONKI852M0Ed",
        "outputId": "96b8459a-b292-4cb4-d5bb-bbe0bb6f9d88",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                            OLS Regression Results                            \n",
              "==============================================================================\n",
              "Dep. Variable:                  estim   R-squared:                       0.102\n",
              "Model:                            OLS   Adj. R-squared:                  0.100\n",
              "Method:                 Least Squares   F-statistic:                     58.28\n",
              "Date:                Wed, 12 Nov 2025   Prob (F-statistic):           1.05e-24\n",
              "Time:                        08:15:44   Log-Likelihood:                -4150.5\n",
              "No. Observations:                1034   AIC:                             8307.\n",
              "Df Residuals:                    1031   BIC:                             8322.\n",
              "Df Model:                           2                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "==============================================================================\n",
              "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "Intercept      3.7653      0.417      9.024      0.000       2.947       4.584\n",
              "x1             0.3311      0.031     10.518      0.000       0.269       0.393\n",
              "x2             0.0117      0.031      0.371      0.711      -0.050       0.073\n",
              "==============================================================================\n",
              "Omnibus:                      241.912   Durbin-Watson:                   1.964\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               66.241\n",
              "Skew:                          -0.368   Prob(JB):                     4.13e-15\n",
              "Kurtosis:                       2.001   Cond. No.                         14.7\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>          <td>estim</td>      <th>  R-squared:         </th> <td>   0.102</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.100</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   58.28</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Wed, 12 Nov 2025</td> <th>  Prob (F-statistic):</th> <td>1.05e-24</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>08:15:44</td>     <th>  Log-Likelihood:    </th> <td> -4150.5</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td>  1034</td>      <th>  AIC:               </th> <td>   8307.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td>  1031</td>      <th>  BIC:               </th> <td>   8322.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th> <td>    3.7653</td> <td>    0.417</td> <td>    9.024</td> <td> 0.000</td> <td>    2.947</td> <td>    4.584</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x1</th>        <td>    0.3311</td> <td>    0.031</td> <td>   10.518</td> <td> 0.000</td> <td>    0.269</td> <td>    0.393</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x2</th>        <td>    0.0117</td> <td>    0.031</td> <td>    0.371</td> <td> 0.711</td> <td>   -0.050</td> <td>    0.073</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>241.912</td> <th>  Durbin-Watson:     </th> <td>   1.964</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>  66.241</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>          <td>-0.368</td>  <th>  Prob(JB):          </th> <td>4.13e-15</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>      <td> 2.001</td>  <th>  Cond. No.          </th> <td>    14.7</td>\n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &      estim       & \\textbf{  R-squared:         } &     0.102   \\\\\n\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.100   \\\\\n\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     58.28   \\\\\n\\textbf{Date:}             & Wed, 12 Nov 2025 & \\textbf{  Prob (F-statistic):} &  1.05e-24   \\\\\n\\textbf{Time:}             &     08:15:44     & \\textbf{  Log-Likelihood:    } &   -4150.5   \\\\\n\\textbf{No. Observations:} &        1034      & \\textbf{  AIC:               } &     8307.   \\\\\n\\textbf{Df Residuals:}     &        1031      & \\textbf{  BIC:               } &     8322.   \\\\\n\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{Intercept} &       3.7653  &        0.417     &     9.024  &         0.000        &        2.947    &        4.584     \\\\\n\\textbf{x1}        &       0.3311  &        0.031     &    10.518  &         0.000        &        0.269    &        0.393     \\\\\n\\textbf{x2}        &       0.0117  &        0.031     &     0.371  &         0.711        &       -0.050    &        0.073     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       & 241.912 & \\textbf{  Durbin-Watson:     } &    1.964  \\\\\n\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &   66.241  \\\\\n\\textbf{Skew:}          &  -0.368 & \\textbf{  Prob(JB):          } & 4.13e-15  \\\\\n\\textbf{Kurtosis:}      &   2.001 & \\textbf{  Cond. No.          } &     14.7  \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# import OLS from statsmodel package\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# select dataframe for subject 14\n",
        "df_singlesubj = df[df.subj == 14]\n",
        "\n",
        "# define model formula and database (point to a dataframe)\n",
        "mod = ols('estim ~ x1 + x2', data=df_singlesubj)\n",
        "\n",
        "# fit the model\n",
        "res = mod.fit()\n",
        "\n",
        "# summary\n",
        "res.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkze7SsEIUND"
      },
      "source": [
        "Now **fit the model with the second stimulus as unique regressor**, again for subject 14."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g0MpNEsCIUNE",
        "outputId": "77d467cc-f48d-4824-96ee-76508015058f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                            OLS Regression Results                            \n",
              "==============================================================================\n",
              "Dep. Variable:                  estim   R-squared:                       0.005\n",
              "Model:                            OLS   Adj. R-squared:                  0.004\n",
              "Method:                 Least Squares   F-statistic:                     5.353\n",
              "Date:                Wed, 12 Nov 2025   Prob (F-statistic):             0.0209\n",
              "Time:                        08:15:44   Log-Likelihood:                -4203.2\n",
              "No. Observations:                1034   AIC:                             8410.\n",
              "Df Residuals:                    1032   BIC:                             8420.\n",
              "Df Model:                           1                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "==============================================================================\n",
              "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "Intercept      3.7583      0.439      8.564      0.000       2.897       4.620\n",
              "x2             0.0751      0.032      2.314      0.021       0.011       0.139\n",
              "==============================================================================\n",
              "Omnibus:                     3035.567   Durbin-Watson:                   1.990\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              121.133\n",
              "Skew:                          -0.520   Prob(JB):                     4.97e-27\n",
              "Kurtosis:                       1.685   Cond. No.                         13.5\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>          <td>estim</td>      <th>  R-squared:         </th> <td>   0.005</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.004</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   5.353</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Wed, 12 Nov 2025</td> <th>  Prob (F-statistic):</th>  <td>0.0209</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>08:15:44</td>     <th>  Log-Likelihood:    </th> <td> -4203.2</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td>  1034</td>      <th>  AIC:               </th> <td>   8410.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td>  1032</td>      <th>  BIC:               </th> <td>   8420.</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th> <td>    3.7583</td> <td>    0.439</td> <td>    8.564</td> <td> 0.000</td> <td>    2.897</td> <td>    4.620</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x2</th>        <td>    0.0751</td> <td>    0.032</td> <td>    2.314</td> <td> 0.021</td> <td>    0.011</td> <td>    0.139</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>3035.567</td> <th>  Durbin-Watson:     </th> <td>   1.990</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 121.133</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>           <td>-0.520</td>  <th>  Prob(JB):          </th> <td>4.97e-27</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>       <td> 1.685</td>  <th>  Cond. No.          </th> <td>    13.5</td>\n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &      estim       & \\textbf{  R-squared:         } &     0.005   \\\\\n\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.004   \\\\\n\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     5.353   \\\\\n\\textbf{Date:}             & Wed, 12 Nov 2025 & \\textbf{  Prob (F-statistic):} &   0.0209    \\\\\n\\textbf{Time:}             &     08:15:44     & \\textbf{  Log-Likelihood:    } &   -4203.2   \\\\\n\\textbf{No. Observations:} &        1034      & \\textbf{  AIC:               } &     8410.   \\\\\n\\textbf{Df Residuals:}     &        1032      & \\textbf{  BIC:               } &     8420.   \\\\\n\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{Intercept} &       3.7583  &        0.439     &     8.564  &         0.000        &        2.897    &        4.620     \\\\\n\\textbf{x2}        &       0.0751  &        0.032     &     2.314  &         0.021        &        0.011    &        0.139     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       & 3035.567 & \\textbf{  Durbin-Watson:     } &    1.990  \\\\\n\\textbf{Prob(Omnibus):} &   0.000  & \\textbf{  Jarque-Bera (JB):  } &  121.133  \\\\\n\\textbf{Skew:}          &  -0.520  & \\textbf{  Prob(JB):          } & 4.97e-27  \\\\\n\\textbf{Kurtosis:}      &   1.685  & \\textbf{  Cond. No.          } &     13.5  \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "mod = ols('estim ~ x2', data=df_singlesubj)\n",
        "res = mod.fit()\n",
        "res.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-7_-ReiIUNE"
      },
      "source": [
        "**Compare the estimated values and p-values of the second stimulus regressor between the two models.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_JlgGcvIUNE"
      },
      "source": [
        "In estim ~ x2: the estimated value for x2 is 0.0751 with p = 0.0210.\n",
        "\n",
        "In estim ~ x1 + x2: the estimated value for x2 is 0.0117 with p = 0.711.\n",
        "\n",
        "So, when x1 is added, the x2 coefficient shrinks (≈0.075 → ≈0.012) and loses significance (p ≈ 0.021 → 0.711)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp2Pt-_1IUNF"
      },
      "source": [
        "**Why did this happen?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBGNrbFWIUNF"
      },
      "source": [
        "Because x1 and x2 are correlated (r ≈ 0.19), the single-predictor model estim ~ x2 lets x2 “soak up” part of the effect that truly belongs to x1. When you add x1, x2 is forced to explain only the variance in estim that is unique to x2 (i.e., after removing what x1 already explains), so its coefficient shrinks and its p-value rises."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLdGoWXTIUNF"
      },
      "source": [
        "# Colinear regressors\n",
        "We now turn to the problem of colinear regressors. A set of $n$ regressors is said to be colinear if there is a **linear combination of them that always gives 0**, for all trials (observations). For example if regressors $(x_1,x_2,x_3)$ (out of all regressor models) are such that $x_1 + 2x_2- 4x_3=0$. Basically it means that these **regressors encode redundant information**. This is a serious problem because there is an equivalence between different sets of weights (actually each set of weights is equivalent to an infinity of other sets), so we cannot provide unique estimates of the weights. The model is then said to be **unidentifiable**.\n",
        "This is illustrated below, where we want to see the impact of the average stimulus (`xavg`) separately for different bins of the value. Such binning is useful for example if we want to check that the impact of the stimulus is really linear, as implicitly assumed if we declare the model ` estim ~ xavg`.\n",
        "\n",
        "First let us bin the values of the average stimulus into 6 different bins, using the function `cut` (in pandas)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "scrolled": true,
        "id": "3NhZgqNbIUNG",
        "outputId": "f834a2c4-6dde-4cad-e3fd-cb32fde3fab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   x1  x2  xavg     estim  subj              x_bin\n",
              "0 -10   0    -5  -5.21220     1      (-6.667, 0.0]\n",
              "1 -10 -10   -10  -8.21768     1  (-13.333, -6.667]\n",
              "2  20  10    15 -17.93416     1     (13.333, 20.0]\n",
              "3 -20  10    -5 -13.42634     1      (-6.667, 0.0]\n",
              "4  20 -10     5   9.88556     1       (0.0, 6.667]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d38fe2b4-0663-4d64-993d-8e6d69523f82\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>xavg</th>\n",
              "      <th>estim</th>\n",
              "      <th>subj</th>\n",
              "      <th>x_bin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-10</td>\n",
              "      <td>0</td>\n",
              "      <td>-5</td>\n",
              "      <td>-5.21220</td>\n",
              "      <td>1</td>\n",
              "      <td>(-6.667, 0.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-10</td>\n",
              "      <td>-10</td>\n",
              "      <td>-10</td>\n",
              "      <td>-8.21768</td>\n",
              "      <td>1</td>\n",
              "      <td>(-13.333, -6.667]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>15</td>\n",
              "      <td>-17.93416</td>\n",
              "      <td>1</td>\n",
              "      <td>(13.333, 20.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-20</td>\n",
              "      <td>10</td>\n",
              "      <td>-5</td>\n",
              "      <td>-13.42634</td>\n",
              "      <td>1</td>\n",
              "      <td>(-6.667, 0.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20</td>\n",
              "      <td>-10</td>\n",
              "      <td>5</td>\n",
              "      <td>9.88556</td>\n",
              "      <td>1</td>\n",
              "      <td>(0.0, 6.667]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d38fe2b4-0663-4d64-993d-8e6d69523f82')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d38fe2b4-0663-4d64-993d-8e6d69523f82 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d38fe2b4-0663-4d64-993d-8e6d69523f82');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# number of bins\n",
        "nBin = 6\n",
        "\n",
        "# add a new variable with binned values (the bin boundaries are defined automatically)\n",
        "df['x_bin'] = pd.cut(df.xavg,nBin)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X91OYiynIUNG"
      },
      "source": [
        "Now, to use these values as regressors, let's **create a different binary variable for each of these bins** (so that for example `bin1`=1 if the value `xavg` falls in the first bin, 0 otherwise). What we are doing is called *one-hot encoding*: defining one binary variable for each value of the categorical variable `x_bin`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "scrolled": true,
        "id": "Vmt3r4e1IUNH",
        "outputId": "e3c6506d-22a0-4ea5-9f8f-cba268d55289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   x1  x2  xavg     estim  subj              x_bin  bin1  bin2  bin3  bin4  \\\n",
              "0 -10   0    -5  -5.21220     1      (-6.667, 0.0]     0     0     1     0   \n",
              "1 -10 -10   -10  -8.21768     1  (-13.333, -6.667]     0     1     0     0   \n",
              "2  20  10    15 -17.93416     1     (13.333, 20.0]     0     0     0     0   \n",
              "3 -20  10    -5 -13.42634     1      (-6.667, 0.0]     0     0     1     0   \n",
              "4  20 -10     5   9.88556     1       (0.0, 6.667]     0     0     0     1   \n",
              "5 -20   0   -10 -14.17070     1  (-13.333, -6.667]     0     1     0     0   \n",
              "6 -20 -20   -20  -8.83053     1  (-20.04, -13.333]     1     0     0     0   \n",
              "7   0  10     5  15.12759     1       (0.0, 6.667]     0     0     0     1   \n",
              "8 -20  10    -5 -11.17976     1      (-6.667, 0.0]     0     0     1     0   \n",
              "9 -10 -20   -15 -12.24974     1  (-20.04, -13.333]     1     0     0     0   \n",
              "\n",
              "   bin5  bin6  \n",
              "0     0     0  \n",
              "1     0     0  \n",
              "2     0     1  \n",
              "3     0     0  \n",
              "4     0     0  \n",
              "5     0     0  \n",
              "6     0     0  \n",
              "7     0     0  \n",
              "8     0     0  \n",
              "9     0     0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-66043e9a-3db1-4091-819b-48bc56e8b394\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>xavg</th>\n",
              "      <th>estim</th>\n",
              "      <th>subj</th>\n",
              "      <th>x_bin</th>\n",
              "      <th>bin1</th>\n",
              "      <th>bin2</th>\n",
              "      <th>bin3</th>\n",
              "      <th>bin4</th>\n",
              "      <th>bin5</th>\n",
              "      <th>bin6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-10</td>\n",
              "      <td>0</td>\n",
              "      <td>-5</td>\n",
              "      <td>-5.21220</td>\n",
              "      <td>1</td>\n",
              "      <td>(-6.667, 0.0]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-10</td>\n",
              "      <td>-10</td>\n",
              "      <td>-10</td>\n",
              "      <td>-8.21768</td>\n",
              "      <td>1</td>\n",
              "      <td>(-13.333, -6.667]</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>15</td>\n",
              "      <td>-17.93416</td>\n",
              "      <td>1</td>\n",
              "      <td>(13.333, 20.0]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-20</td>\n",
              "      <td>10</td>\n",
              "      <td>-5</td>\n",
              "      <td>-13.42634</td>\n",
              "      <td>1</td>\n",
              "      <td>(-6.667, 0.0]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20</td>\n",
              "      <td>-10</td>\n",
              "      <td>5</td>\n",
              "      <td>9.88556</td>\n",
              "      <td>1</td>\n",
              "      <td>(0.0, 6.667]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-20</td>\n",
              "      <td>0</td>\n",
              "      <td>-10</td>\n",
              "      <td>-14.17070</td>\n",
              "      <td>1</td>\n",
              "      <td>(-13.333, -6.667]</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-20</td>\n",
              "      <td>-20</td>\n",
              "      <td>-20</td>\n",
              "      <td>-8.83053</td>\n",
              "      <td>1</td>\n",
              "      <td>(-20.04, -13.333]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>15.12759</td>\n",
              "      <td>1</td>\n",
              "      <td>(0.0, 6.667]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-20</td>\n",
              "      <td>10</td>\n",
              "      <td>-5</td>\n",
              "      <td>-11.17976</td>\n",
              "      <td>1</td>\n",
              "      <td>(-6.667, 0.0]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-10</td>\n",
              "      <td>-20</td>\n",
              "      <td>-15</td>\n",
              "      <td>-12.24974</td>\n",
              "      <td>1</td>\n",
              "      <td>(-20.04, -13.333]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66043e9a-3db1-4091-819b-48bc56e8b394')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-66043e9a-3db1-4091-819b-48bc56e8b394 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-66043e9a-3db1-4091-819b-48bc56e8b394');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# labels for all of the bins\n",
        "bin_label = df['x_bin'].cat.categories\n",
        "\n",
        "# for each bin\n",
        "for i, bin in enumerate(bin_label):\n",
        "\n",
        "    # name of the new variable\n",
        "    lbl = \"bin\"+str(i+1)\n",
        "\n",
        "    # define variable with boolean condition (and multiply by one to convert to 0/1)\n",
        "    df[lbl] = 1 * (df['x_bin'] == bin)\n",
        "\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE42aTmEIUNH"
      },
      "source": [
        "Take a second to check that the variables have been correctly encoded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFwfFUm-IUNI"
      },
      "source": [
        "Now let's pretend to be naive and **fit a linear regression model with all of these binary variables as regressors** (using all subjects at once)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "scrolled": true,
        "id": "ztrNOgorIUNI",
        "outputId": "8c579237-4462-450e-9b3f-da763aa5f9cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                            OLS Regression Results                            \n",
              "==============================================================================\n",
              "Dep. Variable:                  estim   R-squared:                       0.033\n",
              "Model:                            OLS   Adj. R-squared:                  0.033\n",
              "Method:                 Least Squares   F-statistic:                     98.69\n",
              "Date:                Wed, 12 Nov 2025   Prob (F-statistic):          1.23e-102\n",
              "Time:                        08:15:44   Log-Likelihood:                -65698.\n",
              "No. Observations:               14418   AIC:                         1.314e+05\n",
              "Df Residuals:                   14412   BIC:                         1.315e+05\n",
              "Df Model:                           5                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "==============================================================================\n",
              "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "bin1          -5.9361      0.532    -11.166      0.000      -6.978      -4.894\n",
              "bin2          -3.0796      0.531     -5.796      0.000      -4.121      -2.038\n",
              "bin3          -0.4988      0.348     -1.433      0.152      -1.181       0.184\n",
              "bin4           2.8458      0.461      6.175      0.000       1.942       3.749\n",
              "bin5           5.6653      0.531     10.662      0.000       4.624       6.707\n",
              "bin6           7.5739      0.532     14.247      0.000       6.532       8.616\n",
              "==============================================================================\n",
              "Omnibus:                     5050.296   Durbin-Watson:                   1.754\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           292783.194\n",
              "Skew:                           0.878   Prob(JB):                         0.00\n",
              "Kurtosis:                      25.006   Cond. No.                         1.53\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>          <td>estim</td>      <th>  R-squared:         </th> <td>   0.033</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.033</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   98.69</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Wed, 12 Nov 2025</td> <th>  Prob (F-statistic):</th> <td>1.23e-102</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>08:15:44</td>     <th>  Log-Likelihood:    </th> <td> -65698.</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td> 14418</td>      <th>  AIC:               </th> <td>1.314e+05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td> 14412</td>      <th>  BIC:               </th> <td>1.315e+05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "    <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bin1</th> <td>   -5.9361</td> <td>    0.532</td> <td>  -11.166</td> <td> 0.000</td> <td>   -6.978</td> <td>   -4.894</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bin2</th> <td>   -3.0796</td> <td>    0.531</td> <td>   -5.796</td> <td> 0.000</td> <td>   -4.121</td> <td>   -2.038</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bin3</th> <td>   -0.4988</td> <td>    0.348</td> <td>   -1.433</td> <td> 0.152</td> <td>   -1.181</td> <td>    0.184</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bin4</th> <td>    2.8458</td> <td>    0.461</td> <td>    6.175</td> <td> 0.000</td> <td>    1.942</td> <td>    3.749</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bin5</th> <td>    5.6653</td> <td>    0.531</td> <td>   10.662</td> <td> 0.000</td> <td>    4.624</td> <td>    6.707</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bin6</th> <td>    7.5739</td> <td>    0.532</td> <td>   14.247</td> <td> 0.000</td> <td>    6.532</td> <td>    8.616</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>5050.296</td> <th>  Durbin-Watson:     </th>  <td>   1.754</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>292783.194</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>           <td> 0.878</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>       <td>25.006</td>  <th>  Cond. No.          </th>  <td>    1.53</td> \n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &      estim       & \\textbf{  R-squared:         } &     0.033   \\\\\n\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.033   \\\\\n\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     98.69   \\\\\n\\textbf{Date:}             & Wed, 12 Nov 2025 & \\textbf{  Prob (F-statistic):} & 1.23e-102   \\\\\n\\textbf{Time:}             &     08:15:44     & \\textbf{  Log-Likelihood:    } &   -65698.   \\\\\n\\textbf{No. Observations:} &       14418      & \\textbf{  AIC:               } & 1.314e+05   \\\\\n\\textbf{Df Residuals:}     &       14412      & \\textbf{  BIC:               } & 1.315e+05   \\\\\n\\textbf{Df Model:}         &           5      & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n              & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{bin1} &      -5.9361  &        0.532     &   -11.166  &         0.000        &       -6.978    &       -4.894     \\\\\n\\textbf{bin2} &      -3.0796  &        0.531     &    -5.796  &         0.000        &       -4.121    &       -2.038     \\\\\n\\textbf{bin3} &      -0.4988  &        0.348     &    -1.433  &         0.152        &       -1.181    &        0.184     \\\\\n\\textbf{bin4} &       2.8458  &        0.461     &     6.175  &         0.000        &        1.942    &        3.749     \\\\\n\\textbf{bin5} &       5.6653  &        0.531     &    10.662  &         0.000        &        4.624    &        6.707     \\\\\n\\textbf{bin6} &       7.5739  &        0.532     &    14.247  &         0.000        &        6.532    &        8.616     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       & 5050.296 & \\textbf{  Durbin-Watson:     } &     1.754   \\\\\n\\textbf{Prob(Omnibus):} &   0.000  & \\textbf{  Jarque-Bera (JB):  } & 292783.194  \\\\\n\\textbf{Skew:}          &   0.878  & \\textbf{  Prob(JB):          } &      0.00   \\\\\n\\textbf{Kurtosis:}      &  25.006  & \\textbf{  Cond. No.          } &      1.53   \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# define model formula and database (point to a dataframe)\n",
        "mod = mod = ols('estim ~ bin1 + bin2 + bin3 + bin4 + bin5 + bin6 - 1', data=df)\n",
        "\n",
        "# fit the model\n",
        "res_altogether = mod.fit()\n",
        "\n",
        "# The coefficients\n",
        "res_altogether.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgWnNe1_IUNI"
      },
      "source": [
        "Apparently, everything fine... but let's scroll down until the end of the summary with note \\[2\\]. There is one word within this cryptic message that should raise all alarm bells: multicollinearity! Indeed our regressors are colinear. To see this, first reply this question: **in each trial, what is the sum of all 6 bin regressors?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Csm7jiEnIUNJ"
      },
      "source": [
        "For every trial, the sum of the six bin regressors is 1.\n",
        "\n",
        "(Exactly one bin is 1—because each xavg falls into one and only one interval—and the other five are 0. This is why adding an intercept with all six dummies creates perfect multicollinearity: the intercept column is identical to that sum.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oebUZVK4IUNJ"
      },
      "source": [
        "Collinearity (or multicollinearity) appears when a linear combination of a set of regressors is always null. **What linear combination of regressors is always null?** (Hint: remember there's a regressor of ones for the intercept)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKvWUmm_IUNJ"
      },
      "source": [
        "The linear combination that is always zero is:\n",
        "\n",
        "(Intercept)−(bin1+bin2+bin3+bin4+bin5+bin6)=0\n",
        "\n",
        "that is 1−(bin1+bin2+bin3+bin4+bin5+bin6)=0\n",
        "\n",
        "This expresses the perfect collinearity: the intercept (a column of ones) is exactly equal to the sum of the six dummy regressors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8wzrYvyIUNK"
      },
      "source": [
        "Our system of regressors is multicollinear, i.e. redundant, which means that we should not interpret the estimated weights (or p-values). **The weights of a regression model with colinear regressors are meaningless!** (since they cannot be uniquely determined)\n",
        "\n",
        "We are left with no other choice than removing one of the regressors. We will compare two methods for doing so.\n",
        "First, **fit the same model but without the regressor corresponding to the first bin of stimulus value**.\n",
        "\n",
        "Note: you can check that this gives exactly the same as declaring `x_bin` as a categorical regressor into the model using the formula with `C(x_bin)`. In this case, one-hot encoding is implicitly applied when defining the formula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "scrolled": true,
        "id": "d53gefeFIUNK",
        "outputId": "5e5d5321-fb0f-42de-a63e-8d69310b860c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                            OLS Regression Results                            \n",
              "==============================================================================\n",
              "Dep. Variable:                  estim   R-squared:                       0.033\n",
              "Model:                            OLS   Adj. R-squared:                  0.033\n",
              "Method:                 Least Squares   F-statistic:                     98.69\n",
              "Date:                Wed, 12 Nov 2025   Prob (F-statistic):          1.23e-102\n",
              "Time:                        08:15:44   Log-Likelihood:                -65698.\n",
              "No. Observations:               14418   AIC:                         1.314e+05\n",
              "Df Residuals:                   14412   BIC:                         1.315e+05\n",
              "Df Model:                           5                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "==============================================================================\n",
              "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "Intercept     -5.9361      0.532    -11.166      0.000      -6.978      -4.894\n",
              "bin2           2.8565      0.752      3.800      0.000       1.383       4.330\n",
              "bin3           5.4373      0.635      8.556      0.000       4.192       6.683\n",
              "bin4           8.7819      0.704     12.482      0.000       7.403      10.161\n",
              "bin5          11.6014      0.752     15.435      0.000      10.128      13.075\n",
              "bin6          13.5100      0.752     17.969      0.000      12.036      14.984\n",
              "==============================================================================\n",
              "Omnibus:                     5050.296   Durbin-Watson:                   1.754\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           292783.194\n",
              "Skew:                           0.878   Prob(JB):                         0.00\n",
              "Kurtosis:                      25.006   Cond. No.                         7.81\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>          <td>estim</td>      <th>  R-squared:         </th> <td>   0.033</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.033</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   98.69</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Wed, 12 Nov 2025</td> <th>  Prob (F-statistic):</th> <td>1.23e-102</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>08:15:44</td>     <th>  Log-Likelihood:    </th> <td> -65698.</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td> 14418</td>      <th>  AIC:               </th> <td>1.314e+05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td> 14412</td>      <th>  BIC:               </th> <td>1.315e+05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th> <td>   -5.9361</td> <td>    0.532</td> <td>  -11.166</td> <td> 0.000</td> <td>   -6.978</td> <td>   -4.894</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bin2</th>      <td>    2.8565</td> <td>    0.752</td> <td>    3.800</td> <td> 0.000</td> <td>    1.383</td> <td>    4.330</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bin3</th>      <td>    5.4373</td> <td>    0.635</td> <td>    8.556</td> <td> 0.000</td> <td>    4.192</td> <td>    6.683</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bin4</th>      <td>    8.7819</td> <td>    0.704</td> <td>   12.482</td> <td> 0.000</td> <td>    7.403</td> <td>   10.161</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bin5</th>      <td>   11.6014</td> <td>    0.752</td> <td>   15.435</td> <td> 0.000</td> <td>   10.128</td> <td>   13.075</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bin6</th>      <td>   13.5100</td> <td>    0.752</td> <td>   17.969</td> <td> 0.000</td> <td>   12.036</td> <td>   14.984</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>5050.296</td> <th>  Durbin-Watson:     </th>  <td>   1.754</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>292783.194</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>           <td> 0.878</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>       <td>25.006</td>  <th>  Cond. No.          </th>  <td>    7.81</td> \n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &      estim       & \\textbf{  R-squared:         } &     0.033   \\\\\n\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.033   \\\\\n\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     98.69   \\\\\n\\textbf{Date:}             & Wed, 12 Nov 2025 & \\textbf{  Prob (F-statistic):} & 1.23e-102   \\\\\n\\textbf{Time:}             &     08:15:44     & \\textbf{  Log-Likelihood:    } &   -65698.   \\\\\n\\textbf{No. Observations:} &       14418      & \\textbf{  AIC:               } & 1.314e+05   \\\\\n\\textbf{Df Residuals:}     &       14412      & \\textbf{  BIC:               } & 1.315e+05   \\\\\n\\textbf{Df Model:}         &           5      & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{Intercept} &      -5.9361  &        0.532     &   -11.166  &         0.000        &       -6.978    &       -4.894     \\\\\n\\textbf{bin2}      &       2.8565  &        0.752     &     3.800  &         0.000        &        1.383    &        4.330     \\\\\n\\textbf{bin3}      &       5.4373  &        0.635     &     8.556  &         0.000        &        4.192    &        6.683     \\\\\n\\textbf{bin4}      &       8.7819  &        0.704     &    12.482  &         0.000        &        7.403    &       10.161     \\\\\n\\textbf{bin5}      &      11.6014  &        0.752     &    15.435  &         0.000        &       10.128    &       13.075     \\\\\n\\textbf{bin6}      &      13.5100  &        0.752     &    17.969  &         0.000        &       12.036    &       14.984     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       & 5050.296 & \\textbf{  Durbin-Watson:     } &     1.754   \\\\\n\\textbf{Prob(Omnibus):} &   0.000  & \\textbf{  Jarque-Bera (JB):  } & 292783.194  \\\\\n\\textbf{Skew:}          &   0.878  & \\textbf{  Prob(JB):          } &      0.00   \\\\\n\\textbf{Kurtosis:}      &  25.006  & \\textbf{  Cond. No.          } &      7.81   \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# define model formula and database (point to a dataframe)\n",
        "mod_nobin1 = ols('estim ~ bin2 + bin3 + bin4 + bin5 + bin6', data=df)\n",
        "\n",
        "# fit the model\n",
        "res_nobin1 = mod_nobin1.fit()\n",
        "\n",
        "# The coefficients\n",
        "res_nobin1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnk5HpFOIUNL"
      },
      "source": [
        "Another possibility is to remove the fixed regressor that captures the intercept. This regressor is included automatically by default by `ols`, but we can choose not to include it by adding `+0` in the formula. For example, `estim ~ xavg + 0` fits a simple linear regression model without intercept. **Fit the model with all stimulus bin regressors but no intercept.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uou-C9XKIUNL",
        "outputId": "9b9794ba-8901-432d-976c-5b2e253a63a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                            OLS Regression Results                            \n",
              "==============================================================================\n",
              "Dep. Variable:                  estim   R-squared:                       0.033\n",
              "Model:                            OLS   Adj. R-squared:                  0.033\n",
              "Method:                 Least Squares   F-statistic:                     98.69\n",
              "Date:                Wed, 12 Nov 2025   Prob (F-statistic):          1.23e-102\n",
              "Time:                        08:15:45   Log-Likelihood:                -65698.\n",
              "No. Observations:               14418   AIC:                         1.314e+05\n",
              "Df Residuals:                   14412   BIC:                         1.315e+05\n",
              "Df Model:                           5                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "==============================================================================\n",
              "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "bin1          -5.9361      0.532    -11.166      0.000      -6.978      -4.894\n",
              "bin2          -3.0796      0.531     -5.796      0.000      -4.121      -2.038\n",
              "bin3          -0.4988      0.348     -1.433      0.152      -1.181       0.184\n",
              "bin4           2.8458      0.461      6.175      0.000       1.942       3.749\n",
              "bin5           5.6653      0.531     10.662      0.000       4.624       6.707\n",
              "bin6           7.5739      0.532     14.247      0.000       6.532       8.616\n",
              "==============================================================================\n",
              "Omnibus:                     5050.296   Durbin-Watson:                   1.754\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           292783.194\n",
              "Skew:                           0.878   Prob(JB):                         0.00\n",
              "Kurtosis:                      25.006   Cond. No.                         1.53\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>          <td>estim</td>      <th>  R-squared:         </th> <td>   0.033</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.033</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   98.69</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Wed, 12 Nov 2025</td> <th>  Prob (F-statistic):</th> <td>1.23e-102</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>08:15:45</td>     <th>  Log-Likelihood:    </th> <td> -65698.</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td> 14418</td>      <th>  AIC:               </th> <td>1.314e+05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td> 14412</td>      <th>  BIC:               </th> <td>1.315e+05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "    <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bin1</th> <td>   -5.9361</td> <td>    0.532</td> <td>  -11.166</td> <td> 0.000</td> <td>   -6.978</td> <td>   -4.894</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bin2</th> <td>   -3.0796</td> <td>    0.531</td> <td>   -5.796</td> <td> 0.000</td> <td>   -4.121</td> <td>   -2.038</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bin3</th> <td>   -0.4988</td> <td>    0.348</td> <td>   -1.433</td> <td> 0.152</td> <td>   -1.181</td> <td>    0.184</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bin4</th> <td>    2.8458</td> <td>    0.461</td> <td>    6.175</td> <td> 0.000</td> <td>    1.942</td> <td>    3.749</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bin5</th> <td>    5.6653</td> <td>    0.531</td> <td>   10.662</td> <td> 0.000</td> <td>    4.624</td> <td>    6.707</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>bin6</th> <td>    7.5739</td> <td>    0.532</td> <td>   14.247</td> <td> 0.000</td> <td>    6.532</td> <td>    8.616</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>5050.296</td> <th>  Durbin-Watson:     </th>  <td>   1.754</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>292783.194</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>           <td> 0.878</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>       <td>25.006</td>  <th>  Cond. No.          </th>  <td>    1.53</td> \n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &      estim       & \\textbf{  R-squared:         } &     0.033   \\\\\n\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.033   \\\\\n\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     98.69   \\\\\n\\textbf{Date:}             & Wed, 12 Nov 2025 & \\textbf{  Prob (F-statistic):} & 1.23e-102   \\\\\n\\textbf{Time:}             &     08:15:45     & \\textbf{  Log-Likelihood:    } &   -65698.   \\\\\n\\textbf{No. Observations:} &       14418      & \\textbf{  AIC:               } & 1.314e+05   \\\\\n\\textbf{Df Residuals:}     &       14412      & \\textbf{  BIC:               } & 1.315e+05   \\\\\n\\textbf{Df Model:}         &           5      & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n              & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{bin1} &      -5.9361  &        0.532     &   -11.166  &         0.000        &       -6.978    &       -4.894     \\\\\n\\textbf{bin2} &      -3.0796  &        0.531     &    -5.796  &         0.000        &       -4.121    &       -2.038     \\\\\n\\textbf{bin3} &      -0.4988  &        0.348     &    -1.433  &         0.152        &       -1.181    &        0.184     \\\\\n\\textbf{bin4} &       2.8458  &        0.461     &     6.175  &         0.000        &        1.942    &        3.749     \\\\\n\\textbf{bin5} &       5.6653  &        0.531     &    10.662  &         0.000        &        4.624    &        6.707     \\\\\n\\textbf{bin6} &       7.5739  &        0.532     &    14.247  &         0.000        &        6.532    &        8.616     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       & 5050.296 & \\textbf{  Durbin-Watson:     } &     1.754   \\\\\n\\textbf{Prob(Omnibus):} &   0.000  & \\textbf{  Jarque-Bera (JB):  } & 292783.194  \\\\\n\\textbf{Skew:}          &   0.878  & \\textbf{  Prob(JB):          } &      0.00   \\\\\n\\textbf{Kurtosis:}      &  25.006  & \\textbf{  Cond. No.          } &      1.53   \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# define model formula and database (point to a dataframe)\n",
        "mod_nointercept = ols('estim ~ bin1 + bin2 + bin3 + bin4 + bin5 + bin6 + 0', data=df)\n",
        "\n",
        "# fit the model\n",
        "res_nointercept = mod_nointercept.fit()\n",
        "\n",
        "# The coefficients\n",
        "res_nointercept.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8QUnaMNIUNM"
      },
      "source": [
        "Now we can check that that dreadful \"Note \\[2\\]\" is absent from both of these models.\n",
        "Both models are valid and equivalent, but how do their weights compare? Let us **plot these weights in a bar plot** (we will do it by converting weight objects to dataframe and then merging the two dataframes using `join`). Note that the bars are absent when the regressor is missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "j45LwyE2IUNM",
        "outputId": "46ff08c3-7e7b-4aec-daac-0c585ddecb6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# converts weights for model without bin1 to dataframe\n",
        "df1 = pd.DataFrame({'NoBin1': res_nobin1.params})\n",
        "\n",
        "# converts weights for model without intercept to dataframe\n",
        "df2 = pd.DataFrame({'NoIntercept': res_nointercept.params})\n",
        "\n",
        "# join the two dataframes (missing values are filled with nans)\n",
        "df_weights = df1.join(df2,how = 'outer')\n",
        "\n",
        "# plot\n",
        "df_weights.plot.bar();\n",
        "plt.ylabel('weights');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiiE1W7IIUNN"
      },
      "source": [
        "**How do the weights of the two models differ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMV-KmvFIUNO"
      },
      "source": [
        "In both models the predictions are exactly the same; what changes is just how the information is encoded in the coefficients.\n",
        "\n",
        "In the **“NoBin1” model** (with intercept, without `bin1`), the intercept plays the role of the average estimate in the reference bin (bin 1). Each bin coefficient then tells you how much higher or lower that bin’s mean is compared with bin 1. So these weights are *differences relative to bin 1*.\n",
        "\n",
        "In the **“NoIntercept” model** (with all bins, no intercept), each bin coefficient directly corresponds to the mean estimate for that bin. There is no special reference bin; every bar is the raw average for its bin.\n",
        "\n",
        "So visually: the orange bars are just the blue bars “shifted” so that bin 1’s value is no longer hidden in the intercept. The underlying fit to the data does not change—only the reference frame used to express the bin means.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33Cm0C-uIUNO"
      },
      "source": [
        "##  Checking for colinear or highly correlated regressors\n",
        "Because we won't always remember to look for a possible note at the bottom of a summary to check whether our model makes sense, it is good to **perform checks prior to fitting**. Moreover, while colinearity is a fatal issue for regression models (unless we introduce priors or regularizers over weights, but we don't cover these during our course), **strong correlations between regressors can also severely damage our ability to interpret regression results**. Basically, if a linear combination of regressors is not exactly null but close enough to zero for all trials, then regressors can be estimated, but with a very large uncertainty.\n",
        "This is why introducing too many regressors may lead to bad estimation through **overfitting**: as we add more and more regressors, we are bound to find *some* combination of regressors that is close enough to zero. This problem may be alleviated if we have a sufficiently large dataset.\n",
        "\n",
        "The good news is that we can check *a priori* whether regressors are colinear or highly correlated using the **Variance Inflation Factor** (VIF). The VIF is a metric for each regressor in a regression model that determines how much estimation will be affected by correlation with other regressors (technically, how much the variance of the weight is inflated by the presence of the other regressors). Its value is always larger than 1, and the smaller the better. It is 1 if the regressor under study is completely uncorrelated to other regressors. It is infinite when regressors are colinear, and very large if the regressor is strongly correlated to others. A rule of thumb is that *if any VIF is larger than 5 (sometimes we see 10), then the model cannot be properly estimated and one regressor must be excluded.* But this is just a rule of thumb, how severe the problem is depends on the actual dataset. In any case, large VIFs should at minimum raise caution.\n",
        "VIFs can be computed using `variance_inflation_factor` from the `statsmodel` package.\n",
        "**Check the VIF for the full set of stimulus bins as regressors.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "scrolled": true,
        "id": "D0lEZn-dIUNO",
        "outputId": "01dbd4fe-1592-467b-9746-30d12b6adde6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VIF for bin  1 :  0.0\n",
            "VIF for bin  2 :  inf\n",
            "VIF for bin  3 :  inf\n",
            "VIF for bin  4 :  inf\n",
            "VIF for bin  5 :  inf\n",
            "VIF for bin  6 :  inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/regression/linear_model.py:1782: RuntimeWarning: divide by zero encountered in scalar divide\n",
            "  return 1 - self.ssr/self.centered_tss\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
            "  vif = 1. / (1. - r_squared_i)\n"
          ]
        }
      ],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# define the set of regressors we want to test independent variables set\n",
        "X = df[['bin1', 'bin2','bin3','bin4','bin5','bin6']]\n",
        "\n",
        "# convert to array\n",
        "X = X.values\n",
        "\n",
        "# remember to add fixed regressor (for intercept)\n",
        "fixed_regressor = np.ones((X.shape[0], 1)) # 2D array with same number of lines as X and one column\n",
        "X = np.concatenate((fixed_regressor, X), axis=1)\n",
        "\n",
        "vif= []\n",
        "for i in range(nBin): # loop through bins\n",
        "    # compute VIF for corresponding regressor\n",
        "    V = variance_inflation_factor(X, i)\n",
        "    vif.append(V)\n",
        "\n",
        "    print(\"VIF for bin \", i+1, \": \", V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uTHHbsYIUNd"
      },
      "source": [
        "We see that indeed VIF values are infinite, which denotes multicollinearity.\n",
        "**Compute VIF values for the same set of regressors, excluding regressor for the first bin.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FKOMYQTqIUNe",
        "outputId": "e664e122-37ca-40b8-bb0d-6841c84e4f52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VIF for bin  2 :  7.665071770334929\n",
            "VIF for bin  3 :  1.739723125052865\n",
            "VIF for bin  4 :  2.318352133670705\n",
            "VIF for bin  5 :  1.926064272692393\n",
            "VIF for bin  6 :  1.739723125052865\n"
          ]
        }
      ],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# define the set of regressors we want to test independent variables set\n",
        "df_sub = df[['bin2','bin3','bin4','bin5','bin6']]\n",
        "\n",
        "# convert to array\n",
        "X = df_sub.values\n",
        "\n",
        "# remember to add fixed regressor (for intercept)\n",
        "fixed_regressor = np.ones((X.shape[0], 1))\n",
        "X = np.concatenate((fixed_regressor, X), axis=1)\n",
        "\n",
        "vif= []\n",
        "for i in range(nBin-1): # loop through bins\n",
        "    # compute VIF for corresponding regressor\n",
        "    V = variance_inflation_factor(X, i)\n",
        "    vif.append(V)\n",
        "\n",
        "    print(\"VIF for bin \", i+2, \": \", V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL9pH7i4IUNf"
      },
      "source": [
        "**What do you conclude about this set of regressors?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2vqjv3fIUNg"
      },
      "source": [
        "*The values of VIF are close to 2, which is clearly on the safe side (lower than 5). This shows that the model with this set of regressors is well defined.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcvVfCiCIUNg"
      },
      "source": [
        "The function below allows to plot VIF values for variables in a dataframe (the intercept is added within the function)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AslRKwRvIUNh"
      },
      "outputs": [],
      "source": [
        "def plot_vif(df):\n",
        "\n",
        "    # VIF dataframe\n",
        "    VIF_df = pd.DataFrame()\n",
        "    VIF_df[\"regressor\"] = df.columns\n",
        "\n",
        "    # convert to array\n",
        "    X = df.values\n",
        "    nReg = X.shape[1] # number of regressors\n",
        "\n",
        "    # add fixed regressor (for intercept)\n",
        "    fixed_regressor = np.ones((X.shape[0],1))\n",
        "    X = np.concatenate((X,fixed_regressor),axis =1 )\n",
        "\n",
        "    # calculating VIF for each feature\n",
        "    VIF_df[\"VIF\"] = [variance_inflation_factor(X, i)\n",
        "                            for i in range(nReg)]\n",
        "\n",
        "    VIF_df.plot.barh(x='regressor',y='VIF');\n",
        "    plt.xlabel('Variance Inflation Factor');\n",
        "    plt.plot((1,1),(-1,nReg),'r');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dkdvN2GBIUNh",
        "outputId": "f4598533-8456-4893-f559-2eebace98880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_vif(df_sub)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kBqErfoIUNi"
      },
      "source": [
        "# Simulation-based power analysis\n",
        "We are now fast rewinding and putting ourselves in the shoes of the experimenter *before* she actually starts collecting any data (well maybe these are actually your shoes).\n",
        "How can I know **how many subjects to include in my sample to detect the effect of interest (if it is indeed present)**. This is the goal of *power analyses*. Statistical power refers to the probability that the effect, if present, is detected. The larger the better. The larger the sample size the larger the power. But we don't want to collect data from an infinite cohort of subjects either...\n",
        "When your test of interest is a simple t-test or anova, then you can use a software ([G\\*Power](https://www.psychologie.hhu.de/arbeitsgruppen/allgemeine-psychologie-und-arbeitspsychologie/gpower)) or online tools to run your power analyses. When you want to use the result of a more complex statistical analysis (as here), you can used simulation-based power analysis, which is presented in the panel B of the figure below (from [this](https://link.springer.com/article/10.3758/s13428-022-01793-9) little guide to behavioral experiments in humans we wrote)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qqCk3rJIUNi"
      },
      "source": [
        "![Task](https://media.springernature.com/full/springer-static/image/art%3A10.3758%2Fs13428-022-01793-9/MediaObjects/13428_2022_1793_Fig2_HTML.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeKjFSvUIUNj"
      },
      "source": [
        "Here **we will calculate the statistical power for detecting the confirmation bias in the dataset**. We will switch to the numerosity judgment dataset (of the same paper), the one we used in Assignment 2 to test for the confirmation bias.\n",
        "\n",
        "The code below loads the data, changes the reference of the variable to 50, splits the second stimulus into two regressors `x2con` and `x2inc` depending on whether they are consistent with the intermediate binary classification performed by the subject. Just as we did in Assignment 2 (note that you don't really need to delve back into the intricacies of this regression model to follow the logic of the power analysis. It's just a linear regression model, that's about all we need to know.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GzAgFeNcIUNj",
        "outputId": "385f6df8-2084-4f5f-caaf-6abdabacb692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      x1  binchoice      x2    xavg  estim  subj  consistent   x2con   x2inc\n",
              "0 -4.625         -1  -8.375 -6.5000     44     1        True  -8.375  -0.000\n",
              "1 -2.500          1 -11.375 -6.9375     41     1       False  -0.000 -11.375\n",
              "2 -2.250          1   7.500  2.6250     54     1        True   7.500   0.000\n",
              "3 -3.375         -1 -11.625 -7.5000     40     1        True -11.625  -0.000\n",
              "4 -2.750         -1  -8.625 -5.6875     45     1        True  -8.625  -0.000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-38b8bb17-e1e9-43c9-80a3-48fad9471dd6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x1</th>\n",
              "      <th>binchoice</th>\n",
              "      <th>x2</th>\n",
              "      <th>xavg</th>\n",
              "      <th>estim</th>\n",
              "      <th>subj</th>\n",
              "      <th>consistent</th>\n",
              "      <th>x2con</th>\n",
              "      <th>x2inc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-4.625</td>\n",
              "      <td>-1</td>\n",
              "      <td>-8.375</td>\n",
              "      <td>-6.5000</td>\n",
              "      <td>44</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>-8.375</td>\n",
              "      <td>-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-2.500</td>\n",
              "      <td>1</td>\n",
              "      <td>-11.375</td>\n",
              "      <td>-6.9375</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>-11.375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-2.250</td>\n",
              "      <td>1</td>\n",
              "      <td>7.500</td>\n",
              "      <td>2.6250</td>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>7.500</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-3.375</td>\n",
              "      <td>-1</td>\n",
              "      <td>-11.625</td>\n",
              "      <td>-7.5000</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>-11.625</td>\n",
              "      <td>-0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-2.750</td>\n",
              "      <td>-1</td>\n",
              "      <td>-8.625</td>\n",
              "      <td>-5.6875</td>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>-8.625</td>\n",
              "      <td>-0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38b8bb17-e1e9-43c9-80a3-48fad9471dd6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-38b8bb17-e1e9-43c9-80a3-48fad9471dd6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-38b8bb17-e1e9-43c9-80a3-48fad9471dd6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "#load the data\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/wimmerlab/MBC_data_analysis/main/A2_LinearRegression/Task_Numerical.csv\",sep=',')\n",
        "\n",
        "# use 50 as reference point\n",
        "df.x1 = df.x1 - 50\n",
        "df.x2 = df.x2 - 50\n",
        "df.xavg = df.xavg - 50\n",
        "\n",
        "# sign of x2\n",
        "sgn = np.sign(df['x2'])\n",
        "\n",
        "# define boolean array for consistency\n",
        "consistent = sgn ==df['binchoice']\n",
        "\n",
        "# add to dataframe\n",
        "df['consistent'] = consistent\n",
        "\n",
        "# x2con: equal to x2 if consistent, 0 otherwise\n",
        "x2con = df['x2'] * consistent\n",
        "\n",
        "# x2inc: equal to x2 if inconsistent, 0 otherwise\n",
        "x2inc = df['x2'] * ~consistent # could use (1-consistent) instead of ~consistent\n",
        "\n",
        "# add as variables in dataframe\n",
        "df['x2con'] = x2con\n",
        "df['x2inc'] = x2inc\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynqE3S6tIUNk"
      },
      "source": [
        "Now if we want to run a simulation-based power analysis we need to be able to simulate the model. Makes sense. Here I adapted the code we used to simulate the simple linear regression model to the case of multiple regression (so multiple regressors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SVOr9SKfIUNk"
      },
      "outputs": [],
      "source": [
        "#import function\n",
        "from numpy.random import normal\n",
        "\n",
        "# define function that simulates the linear regression model\n",
        "def generate_multiple_linear_model(x, intercept, weights, sigma):\n",
        "    \"\"\"\n",
        "    Simulates a multiple linear model\n",
        "    Args:\n",
        "       X (array): value(s) of the regressor\n",
        "       intercept (float): intercept\n",
        "       weights (array): weights for all regressors\n",
        "       sigma (float): standard deviation of noise parameter\n",
        "    \"\"\"\n",
        "    # use linear relationship (using matrix multiplication)\n",
        "    y = intercept + np.matmul(x,weights)\n",
        "\n",
        "    # add gaussian noise with standard deviation=sigma\n",
        "    n = x.shape[0]\n",
        "    y = y + sigma*normal(size=n)\n",
        "\n",
        "    # output the result\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGWPAnaOIUNl"
      },
      "source": [
        "**Let us simulate the model**. For this, we need to specify model parameters. This is the weak part of power analysis: you need to know the true values of the model parameters to know how likely you are to detect an effect... sounds a bit circular. So, in general we use plausible values, either from the literature or from pilot participants. The most crucial choice is the strength of the effect (so here the difference between the weights of the second stimulus when it's consistent vs. inconsistent with the intermediate choice, `w_x2con` and `w_x2inc`). Needless to say, the larger this difference, the more probable it is to be detected statistically. Here we test with a difference of .1: that is if the weight of the second stimulus on the participant estimate is 0.5 if it is consistent with the first, and 0.4 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ODY7ZaqxIUNl"
      },
      "outputs": [],
      "source": [
        "effect_size = .1 # size of difference between weights of second stimulus depending on its consistency with intermediate choice\n",
        "w_x1 = .5 # weight for the first stimulus\n",
        "w_x2con = .5 # weight for the second stimulus, if consistent\n",
        "w_x2inc = .5 - effect_size # weight for the second stimulus, if inconsistent\n",
        "intercept = 50 # intercept (50 because this is the reference point, corresponds to an unbiased model)\n",
        "\n",
        "sigma = 5 # value of the gaussian noise\n",
        "\n",
        "weights = [w_x1,w_x2con,w_x2inc] # array of weights\n",
        "\n",
        "# define design matrix, i.e. put all regressors into a numpy array\n",
        "X = df[['x1','x2con','x2inc']]\n",
        "X = X.to_numpy()\n",
        "\n",
        "# simulate!\n",
        "y_simul = generate_multiple_linear_model(X, intercept, weights, sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofAwh7yRIUNm"
      },
      "source": [
        "Simulation is the first part of the process. The second part is **fitting the regression model on the simulated data and applying the exact same statistical test that we want to apply on the true experimental data**. Here the test was a paired t-test between the set of consistent/inconsistent weights for the second stimulus, across subjects. I have copied the code from Assignment 2 into a function that takes an experimental dataframe (either from experimental or simulated data) as input and spits out the p-value for the corresponding test. Basically it tells us whether the effect is statistically significant in any dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SnHNF9EzIUNm"
      },
      "outputs": [],
      "source": [
        "# Importing library\n",
        "import scipy.stats as stats\n",
        "\n",
        "# define function that provides p-value for effect of confirmation bias in given dataset\n",
        "def fit_confirmation_bias_model(df):\n",
        "    \"\"\"\n",
        "    Fits the regression model for confirmation bias\n",
        "    Args:\n",
        "       df (dataframe): experimental dataset\n",
        "    \"\"\"\n",
        "\n",
        "    # list of subjects\n",
        "    subjects = np.unique(df.subj)\n",
        "\n",
        "    # create array for weights for each subject (how many weights per subject?)\n",
        "    pars = np.zeros((len(subjects),4))\n",
        "\n",
        "    # loop through all subjects\n",
        "    for i,s in enumerate(subjects):\n",
        "\n",
        "        # dataframe for this subject\n",
        "        df_subj = df[df.subj==s]\n",
        "\n",
        "        # define and fit regression model\n",
        "        mod = ols(formula = 'estim ~ x1 + x2con + x2inc', data=df_subj)\n",
        "        res = mod.fit()\n",
        "\n",
        "        # store values of the parameters\n",
        "        pars[i,:] = res.params\n",
        "\n",
        "    # compute mean across subjects\n",
        "    mean_weights = np.mean(pars,axis=0)\n",
        "\n",
        "    T = stats.ttest_rel(pars[:,2], pars[:,3])\n",
        "\n",
        "    return  T.pvalue, pars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UnRMCw0IUNn"
      },
      "source": [
        "Now let us plug the two processes: we fit the regression models on the synthetic dataset (from simulations) and compute the corresponding p-value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "s_sXQTs1IUNn",
        "outputId": "c604fef2-2c8b-46c6-dd91-6edea46b0238",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p-value 0.18470470081144946\n"
          ]
        }
      ],
      "source": [
        "# copy the experimental dataframe\n",
        "df_simul = df\n",
        "\n",
        "# copy the value of the simulated model as if it were the estimation from the subject\n",
        "df_simul['estim'] = y_simul\n",
        "\n",
        "# compute the pvalue for the confirmation bias on this dataset\n",
        "pval, est_weights = fit_confirmation_bias_model(df_simul)\n",
        "\n",
        "print(\"p-value\", pval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNsfJF-hIUNo"
      },
      "source": [
        "Great! We're almost there. You can simulate the model again by running the corresponding cell above and see that the p-value is different everytime. Sometimes it is significant, but not always. Let us do it in a consistent way: **build a function that repeatedly simulates a synthetic dataset and estimate the significance of the confirmation bias in this synthetic dataset, and outputs the collection of p-values for all simulated datasets**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LCD0Ui31IUNp"
      },
      "outputs": [],
      "source": [
        "# function that simulates and fits model multiple time\n",
        "def simulate_and_fit_confirmation_bias_model(df, intercept, weights, sigma, nSimul=100):\n",
        "    \"\"\"\n",
        "    Simulates and test for the confirmation bias, for a given number of synthetic dataset\n",
        "    Args:\n",
        "       df (dataframe): experimental dataset\n",
        "       intercept (float): value of intercept\n",
        "       weights (array of 3): value of the weights\n",
        "       sigma (float): std of observation noise (gaussian)\n",
        "       nSimul (integer): number of datasets\n",
        "    \"\"\"\n",
        "    # p-values for all datasets\n",
        "    all_pvalues = []\n",
        "    for s in range(nSimul): # for all datasets\n",
        "\n",
        "        # simulate model\n",
        "        y_simul = generate_multiple_linear_model(df[['x1', 'x2con', 'x2inc']].to_numpy(),\n",
        "                                                 intercept, weights, sigma)\n",
        "\n",
        "        # add simulated behavior as dependent variable in the dataframe\n",
        "        df_tmp = df.copy()\n",
        "        df_tmp['estim'] = y_simul\n",
        "\n",
        "        # test for confirmation bias in the dataset\n",
        "        pval, _ = fit_confirmation_bias_model(df_tmp)\n",
        "\n",
        "        # add p-value\n",
        "        all_pvalues.append(pval)\n",
        "\n",
        "    all_pvalues = np.array(all_pvalues) # convert to numpy\n",
        "    return all_pvalues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW3tRFKiIUNp"
      },
      "source": [
        "Ok, let's run this (this should take around a minute or so)! **Plot the histogram of p-values and add a vertical line for the significance criterion (at 0.05)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Dpkn8vL-IUNp",
        "outputId": "a51d52b4-4964-4e74-b235-d5bdaca64f7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "all_p = simulate_and_fit_confirmation_bias_model(df, intercept, weights, sigma)\n",
        "bins = .05*np.arange(21)\n",
        "plt.hist(all_p, bins=bins)\n",
        "plt.vlines(.05, 0, 70, color='r');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEkajqsvIUNq"
      },
      "source": [
        "Finally, **compute the statistical power for this analysis**, i.e. the proportion of simulations that yield a significant effect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NnaAOTCeIUNr",
        "outputId": "93e86f1e-0520-472f-fa84-9e00c41e7b2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "statistical power:  69.0 %\n"
          ]
        }
      ],
      "source": [
        "# define boolean array for significance\n",
        "signif = (all_p < 0.05)\n",
        "\n",
        "# compute power (i.e average of significant datasets)\n",
        "power = np.mean(signif)\n",
        "\n",
        "print(\"statistical power: \", 100*power, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7lDRicIIUNs"
      },
      "source": [
        "**How do you interpret this result?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_G94glsIUNt"
      },
      "source": [
        "Under the exact assumptions you simulated (your chosen effect sizes for x2con vs x2inc, noise level sigma, current numbers of subjects/trials, and the same regression + paired t-test), the analysis would return p < .05 about 73% of the time. In other words, you have ~73% power to detect that consistency effect and ~27% chance of a Type II error (miss) per study.\n",
        "Two caveats:\n",
        "\n",
        "\n",
        "This power applies only to these parameters and design (same nnn subjects/trials, noise, and model). Change any of them and power changes.\n",
        "\n",
        "\n",
        "The p-value histogram piling up near 0 (with many below .05) is what you expect when a real effect of the simulated size is present; the remaining mass above .05 reflects the miss rate from noise/sample variability.\n",
        "\n",
        "\n",
        "If you need ≥80% power, you’d need a larger effect, less noise, or more data (more participants and/or trials)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8PZURkHIUNt"
      },
      "source": [
        "**Check that the statistical power decreases if you decrease the effect size, and increase if your increase it.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0mM6OesIUNu"
      },
      "source": [
        "Mixed models are not very well developed in Python. The R language is better adapted for advanced statistics methods. You have two choices:\n",
        "1) Use the function *mixedlm* in the Python library **statsmodels.formula.api**. Use the function *mixedlm* in the Python library **statsmodels.formula.api**. The syntax is similar to linear regression in **statsmodel**. There are more sophisticated mixed models with the [bambi](https://bambinos.github.io/bambi/) package.\n",
        "\n",
        "2) Use the Python library **pymer4.models**, which gives you access to R functions from within Python. You can then import the *Lmer* package (or other mixed-effects modelling packages) and call it specifying the fixed and random factors in your design (check the documentation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdrZ3vAWIUNu"
      },
      "source": [
        "1) Let us **implement a linear mixed model where the random factor for the intercept of each participant**. This is very similar to a traditional linear regression model with subject-specific biases (i.e. adding `C(subj)` in the formula), except that now we directly estimate the distribution of biases across the population, rather than each subject bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-04T22:46:55.260351Z",
          "start_time": "2022-02-04T22:46:54.971773Z"
        },
        "id": "OawmGQU2M0FF",
        "outputId": "7aab549e-525e-4dda-e0de-d9be74b55c85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
            "  warnings.warn(msg, ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary2.Summary'>\n",
              "\"\"\"\n",
              "         Mixed Linear Model Regression Results\n",
              "========================================================\n",
              "Model:            MixedLM Dependent Variable: estim     \n",
              "No. Observations: 1738    Method:             REML      \n",
              "No. Groups:       21      Scale:              24.3260   \n",
              "Min. group size:  72      Log-Likelihood:     -5244.9273\n",
              "Max. group size:  84      Converged:          Yes       \n",
              "Mean group size:  82.8                                  \n",
              "--------------------------------------------------------\n",
              "             Coef.  Std.Err.    z    P>|z| [0.025 0.975]\n",
              "--------------------------------------------------------\n",
              "Intercept    50.096    0.122 409.139 0.000 49.856 50.336\n",
              "x1            0.519    0.034  15.439 0.000  0.453  0.585\n",
              "x2            0.463    0.017  27.464 0.000  0.430  0.496\n",
              "Group Var     0.000    0.030                            \n",
              "========================================================\n",
              "\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "       <td>Model:</td>       <td>MixedLM</td> <td>Dependent Variable:</td>    <td>estim</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <td>No. Observations:</td>  <td>1738</td>         <td>Method:</td>          <td>REML</td>   \n",
              "</tr>\n",
              "<tr>\n",
              "     <td>No. Groups:</td>      <td>21</td>          <td>Scale:</td>          <td>24.3260</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <td>Min. group size:</td>    <td>72</td>      <td>Log-Likelihood:</td>   <td>-5244.9273</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <td>Max. group size:</td>    <td>84</td>        <td>Converged:</td>          <td>Yes</td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <td>Mean group size:</td>   <td>82.8</td>            <td></td>                <td></td>     \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "      <td></td>       <th>Coef.</th> <th>Std.Err.</th>    <th>z</th>    <th>P>|z|</th> <th>[0.025</th> <th>0.975]</th>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th> <td>50.096</td>   <td>0.122</td>  <td>409.139</td> <td>0.000</td> <td>49.856</td> <td>50.336</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x1</th>         <td>0.519</td>   <td>0.034</td>  <td>15.439</td>  <td>0.000</td>  <td>0.453</td>  <td>0.585</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x2</th>         <td>0.463</td>   <td>0.017</td>  <td>27.464</td>  <td>0.000</td>  <td>0.430</td>  <td>0.496</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Group Var</th>  <td>0.000</td>   <td>0.030</td>     <td></td>       <td></td>       <td></td>       <td></td>   \n",
              "</tr>\n",
              "</table><br/>\n"
            ],
            "text/latex": "\\begin{table}\n\\caption{Mixed Linear Model Regression Results}\n\\label{}\n\\begin{center}\n\\begin{tabular}{llll}\n\\hline\nModel:            & MixedLM & Dependent Variable: & estim       \\\\\nNo. Observations: & 1738    & Method:             & REML        \\\\\nNo. Groups:       & 21      & Scale:              & 24.3260     \\\\\nMin. group size:  & 72      & Log-Likelihood:     & -5244.9273  \\\\\nMax. group size:  & 84      & Converged:          & Yes         \\\\\nMean group size:  & 82.8    &                     &             \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\n\\begin{center}\n\\begin{tabular}{lrrrrrr}\n\\hline\n          &  Coef. & Std.Err. &       z & P$> |$z$|$ & [0.025 & 0.975]  \\\\\n\\hline\nIntercept & 50.096 &    0.122 & 409.139 &       0.000 & 49.856 & 50.336  \\\\\nx1        &  0.519 &    0.034 &  15.439 &       0.000 &  0.453 &  0.585  \\\\\nx2        &  0.463 &    0.017 &  27.464 &       0.000 &  0.430 &  0.496  \\\\\nGroup Var &  0.000 &    0.030 &         &             &        &         \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}\n\\bigskip\n"
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# import package\n",
        "from statsmodels.formula.api import mixedlm\n",
        "\n",
        "# define model:\n",
        "# - the fixed effects are given in the formula as for ols\n",
        "# - the random factor is defined by 'groups' (variable 'subj')\n",
        "# - the random effects (which weights should vary per subject) is defined by re_formula (here, just the intercept so \"~1\")\n",
        "mod_sub = mixedlm(formula='estim ~ x1 + x2', data=df, groups=df['subj'], re_formula=\"~1\")\n",
        "\n",
        "# fit the model\n",
        "res_sub = mod_sub.fit(maxiter=500,method=\"nm\", reml=\"false\")\n",
        "\n",
        "# show summary\n",
        "res_sub.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHKtj6oHIUNv"
      },
      "source": [
        "This model gives us a similar result to what we got when we modeled subjects as fixed factors in Assignment 2. But actually subjects will differ not only because of their overall bias, but also because of a different weighting of stimulus information. So a better model will incorporate this variability, by adding the first and second stimuli as random factors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-04T22:46:56.059655Z",
          "start_time": "2022-02-04T22:46:55.261765Z"
        },
        "id": "i4sRejolIUNw",
        "outputId": "0a8afa51-2f00-4633-d621-1cf3b3876485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
            "  warnings.warn(msg, ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary2.Summary'>\n",
              "\"\"\"\n",
              "          Mixed Linear Model Regression Results\n",
              "==========================================================\n",
              "Model:              MixedLM Dependent Variable: estim     \n",
              "No. Observations:   1738    Method:             REML      \n",
              "No. Groups:         21      Scale:              24.1968   \n",
              "Min. group size:    72      Log-Likelihood:     -5244.1474\n",
              "Max. group size:    84      Converged:          Yes       \n",
              "Mean group size:    82.8                                  \n",
              "----------------------------------------------------------\n",
              "               Coef.  Std.Err.    z    P>|z| [0.025 0.975]\n",
              "----------------------------------------------------------\n",
              "Intercept      50.096    0.125 400.257 0.000 49.851 50.342\n",
              "x1              0.519    0.035  14.714 0.000  0.450  0.588\n",
              "x2              0.463    0.019  24.227 0.000  0.426  0.501\n",
              "Group Var       0.016    0.036                            \n",
              "Group x x1 Cov -0.006    0.008                            \n",
              "x1 Var          0.002    0.003                            \n",
              "Group x x2 Cov -0.005    0.004                            \n",
              "x1 x x2 Cov     0.002    0.001                            \n",
              "x2 Var          0.002    0.001                            \n",
              "==========================================================\n",
              "\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "       <td>Model:</td>       <td>MixedLM</td> <td>Dependent Variable:</td>    <td>estim</td>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <td>No. Observations:</td>  <td>1738</td>         <td>Method:</td>          <td>REML</td>   \n",
              "</tr>\n",
              "<tr>\n",
              "     <td>No. Groups:</td>      <td>21</td>          <td>Scale:</td>          <td>24.1968</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <td>Min. group size:</td>    <td>72</td>      <td>Log-Likelihood:</td>   <td>-5244.1474</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <td>Max. group size:</td>    <td>84</td>        <td>Converged:</td>          <td>Yes</td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <td>Mean group size:</td>   <td>82.8</td>            <td></td>                <td></td>     \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "         <td></td>         <th>Coef.</th> <th>Std.Err.</th>    <th>z</th>    <th>P>|z|</th> <th>[0.025</th> <th>0.975]</th>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th>      <td>50.096</td>   <td>0.125</td>  <td>400.257</td> <td>0.000</td> <td>49.851</td> <td>50.342</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x1</th>              <td>0.519</td>   <td>0.035</td>  <td>14.714</td>  <td>0.000</td>  <td>0.450</td>  <td>0.588</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x2</th>              <td>0.463</td>   <td>0.019</td>  <td>24.227</td>  <td>0.000</td>  <td>0.426</td>  <td>0.501</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Group Var</th>       <td>0.016</td>   <td>0.036</td>     <td></td>       <td></td>       <td></td>       <td></td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Group x x1 Cov</th> <td>-0.006</td>   <td>0.008</td>     <td></td>       <td></td>       <td></td>       <td></td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x1 Var</th>          <td>0.002</td>   <td>0.003</td>     <td></td>       <td></td>       <td></td>       <td></td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Group x x2 Cov</th> <td>-0.005</td>   <td>0.004</td>     <td></td>       <td></td>       <td></td>       <td></td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x1 x x2 Cov</th>     <td>0.002</td>   <td>0.001</td>     <td></td>       <td></td>       <td></td>       <td></td>   \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>x2 Var</th>          <td>0.002</td>   <td>0.001</td>     <td></td>       <td></td>       <td></td>       <td></td>   \n",
              "</tr>\n",
              "</table><br/>\n"
            ],
            "text/latex": "\\begin{table}\n\\caption{Mixed Linear Model Regression Results}\n\\label{}\n\\begin{center}\n\\begin{tabular}{llll}\n\\hline\nModel:            & MixedLM & Dependent Variable: & estim       \\\\\nNo. Observations: & 1738    & Method:             & REML        \\\\\nNo. Groups:       & 21      & Scale:              & 24.1968     \\\\\nMin. group size:  & 72      & Log-Likelihood:     & -5244.1474  \\\\\nMax. group size:  & 84      & Converged:          & Yes         \\\\\nMean group size:  & 82.8    &                     &             \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\n\\begin{center}\n\\begin{tabular}{lrrrrrr}\n\\hline\n               &  Coef. & Std.Err. &       z & P$> |$z$|$ & [0.025 & 0.975]  \\\\\n\\hline\nIntercept      & 50.096 &    0.125 & 400.257 &       0.000 & 49.851 & 50.342  \\\\\nx1             &  0.519 &    0.035 &  14.714 &       0.000 &  0.450 &  0.588  \\\\\nx2             &  0.463 &    0.019 &  24.227 &       0.000 &  0.426 &  0.501  \\\\\nGroup Var      &  0.016 &    0.036 &         &             &        &         \\\\\nGroup x x1 Cov & -0.006 &    0.008 &         &             &        &         \\\\\nx1 Var         &  0.002 &    0.003 &         &             &        &         \\\\\nGroup x x2 Cov & -0.005 &    0.004 &         &             &        &         \\\\\nx1 x x2 Cov    &  0.002 &    0.001 &         &             &        &         \\\\\nx2 Var         &  0.002 &    0.001 &         &             &        &         \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\\end{table}\n\\bigskip\n"
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "from statsmodels.formula.api import mixedlm\n",
        "\n",
        "# same as above, but now the random effects include x1 and x2\n",
        "mod_sub = mixedlm(formula='estim ~ x1 + x2', data=df, groups=df['subj'], re_formula=\"~x1+x2\")\n",
        "res_sub = mod_sub.fit(maxiter=500,method=\"nm\", reml=\"false\")\n",
        "\n",
        "res_sub.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI9raDAyIUNx"
      },
      "source": [
        "2) we can repeat this analysis using the other method, which calls R from Python. Convince yourself that the results are similar (Note: installing pymer 4 may not work on all distributions)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymer4.models import Lmer\n",
        "\n",
        "model = Lmer('estim ~ x1 + x2 + (1 + x1 + x2|subj)', data=df)\n",
        "res = model.fit()\n",
        "print(res)"
      ],
      "metadata": {
        "id": "Z6TTrjtIJGbq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}